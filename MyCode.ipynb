{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>Description</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Fee</th>\n",
       "      <th>...</th>\n",
       "      <th>Name</th>\n",
       "      <th>PetID</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>RescuerID</th>\n",
       "      <th>State</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Type</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>VideoAmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>Nibble</td>\n",
       "      <td>86e1089a3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8480853f516546f6cf33aa88cd76c379</td>\n",
       "      <td>41326</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I just found it alone yesterday near my apartm...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>No Name Yet</td>\n",
       "      <td>6296e909a</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3082c7125d8fb66f7dd4bff4192c8b14</td>\n",
       "      <td>41401</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Brisco</td>\n",
       "      <td>3422e4906</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>fa90fa5b1ee11c86938398b60abc32cb</td>\n",
       "      <td>41326</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>Miko</td>\n",
       "      <td>5842f1ff5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9238e4f44c71a75282e62f7136c6b240</td>\n",
       "      <td>41401</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This handsome yet cute boy is up for adoption....</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Hunter</td>\n",
       "      <td>850a43f90</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>95481e953f8aed9ec3d16fc4509537e8</td>\n",
       "      <td>41326</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AdoptionSpeed  Age  Breed1  Breed2  Color1  Color2  Color3  \\\n",
       "0            2.0    3     299       0       1       7       0   \n",
       "1            0.0    1     265       0       1       2       0   \n",
       "2            3.0    1     307       0       2       7       0   \n",
       "3            2.0    4     307       0       1       2       0   \n",
       "4            2.0    1     307       0       1       0       0   \n",
       "\n",
       "                                         Description  Dewormed  Fee    ...     \\\n",
       "0  Nibble is a 3+ month old ball of cuteness. He ...         2  100    ...      \n",
       "1  I just found it alone yesterday near my apartm...         3    0    ...      \n",
       "2  Their pregnant mother was dumped by her irresp...         1    0    ...      \n",
       "3  Good guard dog, very alert, active, obedience ...         1  150    ...      \n",
       "4  This handsome yet cute boy is up for adoption....         2    0    ...      \n",
       "\n",
       "          Name      PetID  PhotoAmt  Quantity  \\\n",
       "0       Nibble  86e1089a3       1.0         1   \n",
       "1  No Name Yet  6296e909a       2.0         1   \n",
       "2       Brisco  3422e4906       7.0         1   \n",
       "3         Miko  5842f1ff5       8.0         1   \n",
       "4       Hunter  850a43f90       3.0         1   \n",
       "\n",
       "                          RescuerID  State  Sterilized  Type Vaccinated  \\\n",
       "0  8480853f516546f6cf33aa88cd76c379  41326           2     2          2   \n",
       "1  3082c7125d8fb66f7dd4bff4192c8b14  41401           3     2          3   \n",
       "2  fa90fa5b1ee11c86938398b60abc32cb  41326           2     1          1   \n",
       "3  9238e4f44c71a75282e62f7136c6b240  41401           2     1          1   \n",
       "4  95481e953f8aed9ec3d16fc4509537e8  41326           2     1          2   \n",
       "\n",
       "   VideoAmt  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "labels_breed = pd.read_csv('./breed_labels.csv')\n",
    "labels_colors = pd.read_csv('./color_labels.csv')\n",
    "labels_states = pd.read_csv('./state_labels.csv')\n",
    "\n",
    "train = pd.read_csv('./train.csv')\n",
    "train_size = train.shape[0]\n",
    "test = pd.read_csv('./test/test.csv')\n",
    "test_size = test.shape[0]\n",
    "\n",
    "\n",
    "data_all = pd.concat([train, test])\n",
    "data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "      <th>PetID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Name</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>86e1089a3</td>\n",
       "      <td>3</td>\n",
       "      <td>Nibble</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6296e909a</td>\n",
       "      <td>1</td>\n",
       "      <td>No Name Yet</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AdoptionSpeed      PetID  Age         Name  Vaccinated  Fee\n",
       "0            2.0  86e1089a3    3       Nibble           2  100\n",
       "1            0.0  6296e909a    1  No Name Yet           3    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.columns\n",
    "data_all_temp = data_all[['AdoptionSpeed', 'PetID', 'Age', 'Name', 'Vaccinated', 'Fee']]\n",
    "data_all_temp.iloc[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train images files: 58311\n",
      "num of train metadata files: 58311\n",
      "num of train sentiment files: 14442\n",
      "num of test images files: 15040\n",
      "num of test metadata files: 15040\n",
      "num of test sentiment files: 3815\n"
     ]
    }
   ],
   "source": [
    "train_image_files = sorted(glob.glob('./train_images/*.jpg'))\n",
    "train_metadata_files = sorted(glob.glob('./train_metadata/*.json'))\n",
    "train_sentiment_files = sorted(glob.glob('./train_sentiment/*.json'))\n",
    "\n",
    "print('num of train images files: {}'.format(len(train_image_files)))\n",
    "print('num of train metadata files: {}'.format(len(train_metadata_files)))\n",
    "print('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n",
    "\n",
    "\n",
    "test_image_files = sorted(glob.glob('./test_images/*.jpg'))\n",
    "test_metadata_files = sorted(glob.glob('./test_metadata/*.json'))\n",
    "test_sentiment_files = sorted(glob.glob('./test_sentiment/*.json'))\n",
    "\n",
    "print('num of test images files: {}'.format(len(test_image_files)))\n",
    "print('num of test metadata files: {}'.format(len(test_metadata_files)))\n",
    "print('num of test sentiment files: {}'.format(len(test_sentiment_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18941 entries, 0 to 3947\n",
      "Data columns (total 24 columns):\n",
      "AdoptionSpeed    14993 non-null float64\n",
      "Age              18941 non-null int64\n",
      "Breed1           18941 non-null int64\n",
      "Breed2           18941 non-null int64\n",
      "Color1           18941 non-null int64\n",
      "Color2           18941 non-null int64\n",
      "Color3           18941 non-null int64\n",
      "Description      18927 non-null object\n",
      "Dewormed         18941 non-null int64\n",
      "Fee              18941 non-null int64\n",
      "FurLength        18941 non-null int64\n",
      "Gender           18941 non-null int64\n",
      "Health           18941 non-null int64\n",
      "MaturitySize     18941 non-null int64\n",
      "Name             17381 non-null object\n",
      "PetID            18941 non-null object\n",
      "PhotoAmt         18941 non-null float64\n",
      "Quantity         18941 non-null int64\n",
      "RescuerID        18941 non-null object\n",
      "State            18941 non-null int64\n",
      "Sterilized       18941 non-null int64\n",
      "Type             18941 non-null int64\n",
      "Vaccinated       18941 non-null int64\n",
      "VideoAmt         18941 non-null int64\n",
      "dtypes: float64(2), int64(18), object(4)\n",
      "memory usage: 3.6+ MB\n",
      "(14993, 24)\n",
      "(3948, 23)\n"
     ]
    }
   ],
   "source": [
    "data_all.info()\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14652\n",
      "fraction of pets with images: 0.9772560528246516\n",
      "14652\n",
      "fraction of pets with metadata: 0.9772560528246516\n",
      "14442\n",
      "fraction of pets with sentiment: 0.9632495164410058\n",
      "(3948, 1)\n",
      "3821\n",
      "fraction of pets with images: 0.9678318135764944\n",
      "3821\n",
      "fraction of pets with metadata: 0.9678318135764944\n",
      "3815\n",
      "fraction of pets with sentiment: 0.9663120567375887\n"
     ]
    }
   ],
   "source": [
    "## train_data\n",
    "train_df_ids = train[['PetID']]\n",
    "\n",
    "# Images:\n",
    "train_df_imgs = pd.DataFrame(train_image_files)\n",
    "train_df_imgs.columns = ['image_filename']\n",
    "train_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "train_df_imgs['PetID'] = train_imgs_pets\n",
    "print(len(train_imgs_pets.unique()))\n",
    "print('fraction of pets with images: {}'.format(len(train_imgs_pets.unique()) / train_df_ids.shape[0]))\n",
    "\n",
    "# Metadata:\n",
    "train_df_metadata = pd.DataFrame(train_metadata_files)\n",
    "train_df_metadata.columns = ['metadata_filename']\n",
    "train_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "train_df_metadata['PetID'] = train_metadata_pets\n",
    "print(len(train_metadata_pets.unique()))\n",
    "print('fraction of pets with metadata: {}'.format(len(train_metadata_pets.unique()) / train_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# Sentiment:\n",
    "train_df_sentiment = pd.DataFrame(train_sentiment_files)\n",
    "train_df_sentiment.columns = ['sentiment_filename']\n",
    "train_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "train_df_sentiment['PetID'] = train_sentiment_pets\n",
    "print(len(train_sentiment_pets.unique()))\n",
    "print('fraction of pets with sentiment: {}'.format(len(train_sentiment_pets.unique()) / train_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "## test data\n",
    "\n",
    "test_df_ids = test[['PetID']]\n",
    "print(test_df_ids.shape)\n",
    "\n",
    "# Images:\n",
    "test_df_imgs = pd.DataFrame(test_image_files)\n",
    "test_df_imgs.columns = ['image_filename']\n",
    "test_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "test_df_imgs['PetID'] = test_imgs_pets\n",
    "print(len(test_imgs_pets.unique()))\n",
    "print('fraction of pets with images: {}'.format(len(test_imgs_pets.unique()) / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "# Metadata:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_metadata = pd.DataFrame(test_metadata_files)\n",
    "test_df_metadata.columns = ['metadata_filename']\n",
    "test_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "test_df_metadata['PetID'] = test_metadata_pets\n",
    "print(len(test_metadata_pets.unique()))\n",
    "print('fraction of pets with metadata: {}'.format(len(test_metadata_pets.unique()) / test_df_ids.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "# Sentiment:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_sentiment = pd.DataFrame(test_sentiment_files)\n",
    "test_df_sentiment.columns = ['sentiment_filename']\n",
    "test_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "test_df_sentiment['PetID'] = test_metadata_pets\n",
    "print(len(test_sentiment_pets.unique()))\n",
    "print('fraction of pets with sentiment: {}'.format(len(test_sentiment_pets.unique()) / test_df_ids.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PetFinderParser(object):\n",
    "    \n",
    "    def __init__(self, debug=False):\n",
    "        \n",
    "        self.debug = debug\n",
    "        self.sentence_sep = ' '\n",
    "        \n",
    "        # Does not have to be extracted because main DF already contains description\n",
    "        self.extract_sentiment_text = False\n",
    "        \n",
    "        \n",
    "    def open_metadata_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load metadata file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f:\n",
    "            metadata_file = json.load(f)\n",
    "        return metadata_file\n",
    "            \n",
    "    def open_sentiment_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load sentiment file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as f:\n",
    "            sentiment_file = json.load(f)\n",
    "        return sentiment_file\n",
    "            \n",
    "    def open_image_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load image file.\n",
    "        \"\"\"\n",
    "        image = np.asarray(Image.open(filename))\n",
    "        return image\n",
    "        \n",
    "    def parse_sentiment_file(self, file):\n",
    "        \"\"\"\n",
    "        Parse sentiment file. Output DF with sentiment features.\n",
    "        \"\"\"\n",
    "        \n",
    "        file_sentiment = file['documentSentiment']\n",
    "        file_entities = [x['name'] for x in file['entities']]\n",
    "        file_entities = self.sentence_sep.join(file_entities)\n",
    "\n",
    "        if self.extract_sentiment_text:\n",
    "            file_sentences_text = [x['text']['content'] for x in file['sentences']]\n",
    "            file_sentences_text = self.sentence_sep.join(file_sentences_text)\n",
    "        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n",
    "        \n",
    "        file_sentences_sentiment = pd.DataFrame.from_dict(\n",
    "            file_sentences_sentiment, orient='columns').sum()\n",
    "        file_sentences_sentiment = file_sentences_sentiment.add_prefix('document_').to_dict()\n",
    "        \n",
    "        file_sentiment.update(file_sentences_sentiment)\n",
    "        \n",
    "        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n",
    "        if self.extract_sentiment_text:\n",
    "            df_sentiment['text'] = file_sentences_text\n",
    "            \n",
    "        df_sentiment['entities'] = file_entities\n",
    "        df_sentiment = df_sentiment.add_prefix('sentiment_')\n",
    "        \n",
    "        return df_sentiment\n",
    "    \n",
    "    def parse_metadata_file(self, file):\n",
    "        \"\"\"\n",
    "        Parse metadata file. Output DF with metadata features.\n",
    "        \"\"\"\n",
    "        \n",
    "        file_keys = list(file.keys())\n",
    "        \n",
    "        if 'labelAnnotations' in file_keys:\n",
    "            file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.3)]\n",
    "            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n",
    "            file_top_desc = [x['description'] for x in file_annots]\n",
    "        else:\n",
    "            file_top_score = np.nan\n",
    "            file_top_desc = ['']\n",
    "        \n",
    "        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "        file_crops = file['cropHintsAnnotation']['cropHints']\n",
    "\n",
    "        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "\n",
    "        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n",
    "        \n",
    "        if 'importanceFraction' in file_crops[0].keys():\n",
    "            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n",
    "        else:\n",
    "            file_crop_importance = np.nan\n",
    "\n",
    "        df_metadata = {\n",
    "            'annots_score': file_top_score,\n",
    "            'color_score': file_color_score,\n",
    "            'color_pixelfrac': file_color_pixelfrac,\n",
    "            'crop_conf': file_crop_conf,\n",
    "            'crop_importance': file_crop_importance,\n",
    "            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n",
    "        }\n",
    "        \n",
    "        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n",
    "        df_metadata = df_metadata.add_prefix('metadata_')\n",
    "        \n",
    "        return df_metadata\n",
    "    \n",
    "\n",
    "# Helper function for parallel data processing:\n",
    "def extract_additional_features(pet_id, mode='train'):\n",
    "    \n",
    "    sentiment_filename = './{}_sentiment/{}.json'.format(mode, pet_id)\n",
    "    try:\n",
    "        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n",
    "        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n",
    "        df_sentiment['PetID'] = pet_id\n",
    "    except FileNotFoundError:\n",
    "        df_sentiment = []\n",
    "\n",
    "    dfs_metadata = []\n",
    "    metadata_filenames = sorted(glob.glob('./{}_metadata/{}*.json'.format(mode, pet_id)))\n",
    "    if len(metadata_filenames) > 0:\n",
    "        for f in metadata_filenames:\n",
    "            metadata_file = pet_parser.open_metadata_file(f)\n",
    "            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n",
    "            df_metadata['PetID'] = pet_id\n",
    "            dfs_metadata.append(df_metadata)\n",
    "        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n",
    "    dfs = [df_sentiment, dfs_metadata]\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "\n",
    "pet_parser = PetFinderParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fenny market Old Klang Rd home Lai of Paws Mission breed mongrel companion children couple Dalmation adoption details noise'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pet_parser.open_sentiment_file('./train_sentiment/0a0e8c15b.json')\n",
    "file_entities = [x['name'] for x in file['entities']]\n",
    "file_entities = ' '.join(file_entities)\n",
    "file_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unique IDs from train and test:\n",
    "debug = False\n",
    "train_pet_ids = train.PetID.unique()\n",
    "test_pet_ids = test.PetID.unique()\n",
    "\n",
    "if debug:\n",
    "    train_pet_ids = train_pet_ids[:1000]\n",
    "    test_pet_ids = test_pet_ids[:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=6)]: Done 1238 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=6)]: Done 1788 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=6)]: Done 2438 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=6)]: Done 3188 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=6)]: Done 4038 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=6)]: Done 4988 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=6)]: Done 6038 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=6)]: Done 7188 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=6)]: Done 8438 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=6)]: Done 9788 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=6)]: Done 11238 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=6)]: Done 12788 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=6)]: Done 14438 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=6)]: Done 14993 out of 14993 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14442, 6) (58311, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done 172 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=6)]: Done 1372 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=6)]: Done 3372 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=6)]: Done 3948 out of 3948 | elapsed:   35.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3815, 6) (15040, 7)\n"
     ]
    }
   ],
   "source": [
    "# Train set:\n",
    "# Parallel processing of data:\n",
    "# dfs_train = (extract_additional_features(i, mode='train') for i in train_pet_ids)\n",
    "dfs_train = Parallel(n_jobs=6, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n",
    "\n",
    "# Extract processed data and format them as DFs:\n",
    "train_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\n",
    "train_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n",
    "\n",
    "train_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\n",
    "train_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "print(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n",
    "\n",
    "\n",
    "# Test set:\n",
    "# Parallel processing of data:\n",
    "dfs_test = Parallel(n_jobs=6, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n",
    "\n",
    "# Extract processed data and format them as DFs:\n",
    "test_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\n",
    "test_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n",
    "\n",
    "test_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\n",
    "test_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "print(test_dfs_sentiment.shape, test_dfs_metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_magnitude</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_document_magnitude</th>\n",
       "      <th>sentiment_document_score</th>\n",
       "      <th>sentiment_entities</th>\n",
       "      <th>PetID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Nibble cuteness clinic cats result kitty coupl...</td>\n",
       "      <td>86e1089a3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_magnitude  sentiment_score  sentiment_document_magnitude  \\\n",
       "0                  2.4              0.3                           2.2   \n",
       "\n",
       "   sentiment_document_score  \\\n",
       "0                       1.8   \n",
       "\n",
       "                                  sentiment_entities      PetID  \n",
       "0  Nibble cuteness clinic cats result kitty coupl...  86e1089a3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata_annots_score</th>\n",
       "      <th>metadata_color_score</th>\n",
       "      <th>metadata_color_pixelfrac</th>\n",
       "      <th>metadata_crop_conf</th>\n",
       "      <th>metadata_crop_importance</th>\n",
       "      <th>metadata_annots_top_desc</th>\n",
       "      <th>PetID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.0748377</td>\n",
       "      <td>0.066331</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>cat black cat</td>\n",
       "      <td>86e1089a3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  metadata_annots_score metadata_color_score metadata_color_pixelfrac  \\\n",
       "0              0.973658            0.0748377                 0.066331   \n",
       "\n",
       "  metadata_crop_conf metadata_crop_importance metadata_annots_top_desc  \\\n",
       "0                0.8                        1            cat black cat   \n",
       "\n",
       "       PetID  \n",
       "0  86e1089a3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extend aggregates and improve column naming\n",
    "aggregates = ['mean', 'sum']\n",
    "\n",
    "\n",
    "# Train\n",
    "train_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "train_metadata_desc = train_metadata_desc.reset_index()\n",
    "train_metadata_desc[\n",
    "    'metadata_annots_top_desc'] = train_metadata_desc[\n",
    "    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'metadata'\n",
    "train_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\n",
    "for i in train_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\n",
    "train_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "train_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in train_metadata_gr.columns.tolist()])\n",
    "train_metadata_gr = train_metadata_gr.reset_index()\n",
    "\n",
    "\n",
    "train_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "train_sentiment_desc = train_sentiment_desc.reset_index()\n",
    "train_sentiment_desc[\n",
    "    'sentiment_entities'] = train_sentiment_desc[\n",
    "    'sentiment_entities'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'sentiment'\n",
    "train_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\n",
    "for i in train_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\n",
    "train_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(aggregates)\n",
    "train_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in train_sentiment_gr.columns.tolist()])\n",
    "train_sentiment_gr = train_sentiment_gr.reset_index()\n",
    "\n",
    "\n",
    "# Test\n",
    "test_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "test_metadata_desc = test_metadata_desc.reset_index()\n",
    "test_metadata_desc[\n",
    "    'metadata_annots_top_desc'] = test_metadata_desc[\n",
    "    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'metadata'\n",
    "test_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\n",
    "for i in test_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\n",
    "test_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "test_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in test_metadata_gr.columns.tolist()])\n",
    "test_metadata_gr = test_metadata_gr.reset_index()\n",
    "\n",
    "\n",
    "test_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "test_sentiment_desc = test_sentiment_desc.reset_index()\n",
    "test_sentiment_desc[\n",
    "    'sentiment_entities'] = test_sentiment_desc[\n",
    "    'sentiment_entities'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'sentiment'\n",
    "test_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\n",
    "for i in test_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\n",
    "test_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(aggregates)\n",
    "test_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in test_sentiment_gr.columns.tolist()])\n",
    "test_sentiment_gr = test_sentiment_gr.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 44) (3948, 43)\n"
     ]
    }
   ],
   "source": [
    "# Train merges:\n",
    "train_proc = train.copy()\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_gr, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_gr, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_desc, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "# Test merges:\n",
    "test_proc = test.copy()\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_desc, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "\n",
    "print(train_proc.shape, test_proc.shape)\n",
    "assert train_proc.shape[0] == train.shape[0]\n",
    "assert test_proc.shape[0] == test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 48) (3948, 47)\n"
     ]
    }
   ],
   "source": [
    "train_breed_main = train_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "\n",
    "train_breed_main = train_breed_main.iloc[:, 2:]\n",
    "train_breed_main = train_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "train_breed_second = train_proc[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "\n",
    "train_breed_second = train_breed_second.iloc[:, 2:]\n",
    "train_breed_second = train_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "\n",
    "train_proc = pd.concat(\n",
    "    [train_proc, train_breed_main, train_breed_second], axis=1)\n",
    "\n",
    "\n",
    "test_breed_main = test_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "\n",
    "test_breed_main = test_breed_main.iloc[:, 2:]\n",
    "test_breed_main = test_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "test_breed_second = test_proc[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "\n",
    "test_breed_second = test_breed_second.iloc[:, 2:]\n",
    "test_breed_second = test_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "\n",
    "test_proc = pd.concat(\n",
    "    [test_proc, test_breed_main, test_breed_second], axis=1)\n",
    "\n",
    "print(train_proc.shape, test_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\n",
    "# Copy original X DF for easier experimentation,\n",
    "# all feature engineering will be performed on this one:\n",
    "X_temp = X.copy()\n",
    "\n",
    "\n",
    "# Select subsets of columns:\n",
    "text_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\n",
    "categorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n",
    "\n",
    "# Names are all unique, so they can be dropped by default\n",
    "# Same goes for PetID, it shouldn't be used as a feature\n",
    "to_drop_columns = ['PetID', 'Name', 'RescuerID']\n",
    "# RescuerID will also be dropped, as a feature based on this column will be extracted independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count RescuerID occurrences:\n",
    "rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\n",
    "rescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n",
    "\n",
    "# Merge as another feature onto main DF:\n",
    "X_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Factorize categorical columns:\n",
    "for i in categorical_columns:\n",
    "    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]\n",
    "    \n",
    "# Subset text features:\n",
    "X_text = X_temp[text_columns]\n",
    "\n",
    "for i in X_text.columns:\n",
    "    X_text.loc[:, i] = X_text.loc[:, i].fillna('<MISSING>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating features from: Description\n",
      "generating features from: metadata_annots_top_desc\n",
      "generating features from: sentiment_entities\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "\n",
    "n_components = 5\n",
    "text_features = []\n",
    "\n",
    "\n",
    "# Generate text features:\n",
    "for i in X_text.columns:\n",
    "    \n",
    "    # Initialize decomposition methods:\n",
    "    print('generating features from: {}'.format(i))\n",
    "    svd_ = TruncatedSVD(\n",
    "        n_components=n_components, random_state=1337)\n",
    "    nmf_ = NMF(\n",
    "        n_components=n_components, random_state=1337)\n",
    "    \n",
    "    tfidf_col = TfidfVectorizer().fit_transform(X_text.loc[:, i].values)\n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col)\n",
    "    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n",
    "    \n",
    "    nmf_col = nmf_.fit_transform(tfidf_col)\n",
    "    nmf_col = pd.DataFrame(nmf_col)\n",
    "    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n",
    "    \n",
    "    text_features.append(svd_col)\n",
    "    text_features.append(nmf_col)\n",
    "\n",
    "    \n",
    "# Combine all extracted features:\n",
    "text_features = pd.concat(text_features, axis=1)\n",
    "\n",
    "# Concatenate with main DF:\n",
    "X_temp = pd.concat([X_temp, text_features], axis=1)\n",
    "\n",
    "# Remove raw text columns:\n",
    "for i in X_text.columns:\n",
    "    X_temp = X_temp.drop(i, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (18941, 73)\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns:\n",
    "X_temp = X_temp.drop(to_drop_columns, axis=1)\n",
    "\n",
    "# Check final df shape:\n",
    "print('X shape: {}'.format(X_temp.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14993, 73)\n",
      "X_test shape: (3948, 72)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test again:\n",
    "X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "X_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "\n",
    "# Remove missing target column from test:\n",
    "X_test = X_test.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "\n",
    "assert X_train.shape[0] == train.shape[0]\n",
    "assert X_test.shape[0] == test.shape[0]\n",
    "\n",
    "\n",
    "# Check if columns between the two DFs are the same:\n",
    "train_cols = X_train.columns.tolist()\n",
    "train_cols.remove('AdoptionSpeed')\n",
    "\n",
    "test_cols = X_test.columns.tolist()\n",
    "\n",
    "assert np.all(train_cols == test_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "\n",
    "\n",
    "# FROM: https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n",
    "\n",
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "    \n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.85,\n",
    "          'feature_fraction': 0.8,\n",
    "          'min_split_gain': 0.02,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.0475,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 17}\n",
    "\n",
    "# Additional parameters:\n",
    "early_stop = 500\n",
    "verbose_eval = 100\n",
    "num_rounds = 10000\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_tr distribution: Counter({4.0: 3357, 2.0: 3229, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.05981\tvalid_1's rmse: 1.0949\n",
      "[200]\ttraining's rmse: 1.00942\tvalid_1's rmse: 1.06987\n",
      "[300]\ttraining's rmse: 0.976766\tvalid_1's rmse: 1.0598\n",
      "[400]\ttraining's rmse: 0.952989\tvalid_1's rmse: 1.0529\n",
      "[500]\ttraining's rmse: 0.933968\tvalid_1's rmse: 1.04875\n",
      "[600]\ttraining's rmse: 0.919382\tvalid_1's rmse: 1.0459\n",
      "[700]\ttraining's rmse: 0.903607\tvalid_1's rmse: 1.04333\n",
      "[800]\ttraining's rmse: 0.889744\tvalid_1's rmse: 1.04193\n",
      "[900]\ttraining's rmse: 0.878337\tvalid_1's rmse: 1.04101\n",
      "[1000]\ttraining's rmse: 0.867596\tvalid_1's rmse: 1.04\n",
      "[1100]\ttraining's rmse: 0.857199\tvalid_1's rmse: 1.03926\n",
      "[1200]\ttraining's rmse: 0.847319\tvalid_1's rmse: 1.0387\n",
      "[1300]\ttraining's rmse: 0.836784\tvalid_1's rmse: 1.03807\n",
      "[1400]\ttraining's rmse: 0.826728\tvalid_1's rmse: 1.03799\n",
      "[1500]\ttraining's rmse: 0.817824\tvalid_1's rmse: 1.03724\n",
      "[1600]\ttraining's rmse: 0.808686\tvalid_1's rmse: 1.03715\n",
      "[1700]\ttraining's rmse: 0.799706\tvalid_1's rmse: 1.03679\n",
      "[1800]\ttraining's rmse: 0.789399\tvalid_1's rmse: 1.03668\n",
      "[1900]\ttraining's rmse: 0.780369\tvalid_1's rmse: 1.0365\n",
      "[2000]\ttraining's rmse: 0.770617\tvalid_1's rmse: 1.03662\n",
      "[2100]\ttraining's rmse: 0.76311\tvalid_1's rmse: 1.03653\n",
      "[2200]\ttraining's rmse: 0.755533\tvalid_1's rmse: 1.03677\n",
      "[2300]\ttraining's rmse: 0.748354\tvalid_1's rmse: 1.0368\n",
      "[2400]\ttraining's rmse: 0.741751\tvalid_1's rmse: 1.03681\n",
      "[2500]\ttraining's rmse: 0.734668\tvalid_1's rmse: 1.03701\n",
      "[2600]\ttraining's rmse: 0.727466\tvalid_1's rmse: 1.0374\n",
      "Early stopping, best iteration is:\n",
      "[2124]\ttraining's rmse: 0.761509\tvalid_1's rmse: 1.03641\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3357, 2.0: 3229, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.06244\tvalid_1's rmse: 1.09303\n",
      "[200]\ttraining's rmse: 1.01199\tvalid_1's rmse: 1.07124\n",
      "[300]\ttraining's rmse: 0.977599\tvalid_1's rmse: 1.06268\n",
      "[400]\ttraining's rmse: 0.953535\tvalid_1's rmse: 1.05873\n",
      "[500]\ttraining's rmse: 0.935823\tvalid_1's rmse: 1.05664\n",
      "[600]\ttraining's rmse: 0.919459\tvalid_1's rmse: 1.05544\n",
      "[700]\ttraining's rmse: 0.904265\tvalid_1's rmse: 1.05462\n",
      "[800]\ttraining's rmse: 0.889066\tvalid_1's rmse: 1.05418\n",
      "[900]\ttraining's rmse: 0.874394\tvalid_1's rmse: 1.05383\n",
      "[1000]\ttraining's rmse: 0.862926\tvalid_1's rmse: 1.05357\n",
      "[1100]\ttraining's rmse: 0.851288\tvalid_1's rmse: 1.05301\n",
      "[1200]\ttraining's rmse: 0.840062\tvalid_1's rmse: 1.05271\n",
      "[1300]\ttraining's rmse: 0.828624\tvalid_1's rmse: 1.05221\n",
      "[1400]\ttraining's rmse: 0.817551\tvalid_1's rmse: 1.0523\n",
      "[1500]\ttraining's rmse: 0.807299\tvalid_1's rmse: 1.0523\n",
      "[1600]\ttraining's rmse: 0.797216\tvalid_1's rmse: 1.05267\n",
      "[1700]\ttraining's rmse: 0.787383\tvalid_1's rmse: 1.05288\n",
      "Early stopping, best iteration is:\n",
      "[1292]\ttraining's rmse: 0.829644\tvalid_1's rmse: 1.05218\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3230, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.06306\tvalid_1's rmse: 1.08663\n",
      "[200]\ttraining's rmse: 1.01153\tvalid_1's rmse: 1.0607\n",
      "[300]\ttraining's rmse: 0.979294\tvalid_1's rmse: 1.05095\n",
      "[400]\ttraining's rmse: 0.956558\tvalid_1's rmse: 1.04695\n",
      "[500]\ttraining's rmse: 0.93861\tvalid_1's rmse: 1.0445\n",
      "[600]\ttraining's rmse: 0.921082\tvalid_1's rmse: 1.04349\n",
      "[700]\ttraining's rmse: 0.906066\tvalid_1's rmse: 1.04194\n",
      "[800]\ttraining's rmse: 0.892115\tvalid_1's rmse: 1.0408\n",
      "[900]\ttraining's rmse: 0.878552\tvalid_1's rmse: 1.04003\n",
      "[1000]\ttraining's rmse: 0.865651\tvalid_1's rmse: 1.03932\n",
      "[1100]\ttraining's rmse: 0.85288\tvalid_1's rmse: 1.03928\n",
      "[1200]\ttraining's rmse: 0.841922\tvalid_1's rmse: 1.0395\n",
      "[1300]\ttraining's rmse: 0.831013\tvalid_1's rmse: 1.03972\n",
      "[1400]\ttraining's rmse: 0.819898\tvalid_1's rmse: 1.03998\n",
      "[1500]\ttraining's rmse: 0.809434\tvalid_1's rmse: 1.04029\n",
      "[1600]\ttraining's rmse: 0.799824\tvalid_1's rmse: 1.04034\n",
      "Early stopping, best iteration is:\n",
      "[1124]\ttraining's rmse: 0.849749\tvalid_1's rmse: 1.03908\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3230, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.06184\tvalid_1's rmse: 1.09235\n",
      "[200]\ttraining's rmse: 1.01107\tvalid_1's rmse: 1.07003\n",
      "[300]\ttraining's rmse: 0.978839\tvalid_1's rmse: 1.05998\n",
      "[400]\ttraining's rmse: 0.954236\tvalid_1's rmse: 1.05446\n",
      "[500]\ttraining's rmse: 0.935456\tvalid_1's rmse: 1.05141\n",
      "[600]\ttraining's rmse: 0.918146\tvalid_1's rmse: 1.04956\n",
      "[700]\ttraining's rmse: 0.903053\tvalid_1's rmse: 1.04786\n",
      "[800]\ttraining's rmse: 0.889561\tvalid_1's rmse: 1.04671\n",
      "[900]\ttraining's rmse: 0.877592\tvalid_1's rmse: 1.04611\n",
      "[1000]\ttraining's rmse: 0.86628\tvalid_1's rmse: 1.04602\n",
      "[1100]\ttraining's rmse: 0.855079\tvalid_1's rmse: 1.04599\n",
      "[1200]\ttraining's rmse: 0.844388\tvalid_1's rmse: 1.04563\n",
      "[1300]\ttraining's rmse: 0.833698\tvalid_1's rmse: 1.04536\n",
      "[1400]\ttraining's rmse: 0.823747\tvalid_1's rmse: 1.04537\n",
      "[1500]\ttraining's rmse: 0.814593\tvalid_1's rmse: 1.04518\n",
      "[1600]\ttraining's rmse: 0.805801\tvalid_1's rmse: 1.0452\n",
      "[1700]\ttraining's rmse: 0.796317\tvalid_1's rmse: 1.04539\n",
      "[1800]\ttraining's rmse: 0.787734\tvalid_1's rmse: 1.04598\n",
      "[1900]\ttraining's rmse: 0.777875\tvalid_1's rmse: 1.04608\n",
      "[2000]\ttraining's rmse: 0.76917\tvalid_1's rmse: 1.04625\n",
      "Early stopping, best iteration is:\n",
      "[1567]\ttraining's rmse: 0.808294\tvalid_1's rmse: 1.04507\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3230, 3.0: 2608, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.06081\tvalid_1's rmse: 1.09669\n",
      "[200]\ttraining's rmse: 1.00981\tvalid_1's rmse: 1.07405\n",
      "[300]\ttraining's rmse: 0.977352\tvalid_1's rmse: 1.06383\n",
      "[400]\ttraining's rmse: 0.95379\tvalid_1's rmse: 1.05715\n",
      "[500]\ttraining's rmse: 0.935107\tvalid_1's rmse: 1.05317\n",
      "[600]\ttraining's rmse: 0.918807\tvalid_1's rmse: 1.05092\n",
      "[700]\ttraining's rmse: 0.904573\tvalid_1's rmse: 1.04918\n",
      "[800]\ttraining's rmse: 0.890184\tvalid_1's rmse: 1.04807\n",
      "[900]\ttraining's rmse: 0.876019\tvalid_1's rmse: 1.04718\n",
      "[1000]\ttraining's rmse: 0.862035\tvalid_1's rmse: 1.04641\n",
      "[1100]\ttraining's rmse: 0.847577\tvalid_1's rmse: 1.04535\n",
      "[1200]\ttraining's rmse: 0.836602\tvalid_1's rmse: 1.04494\n",
      "[1300]\ttraining's rmse: 0.825603\tvalid_1's rmse: 1.04477\n",
      "[1400]\ttraining's rmse: 0.81574\tvalid_1's rmse: 1.04477\n",
      "[1500]\ttraining's rmse: 0.807707\tvalid_1's rmse: 1.04496\n",
      "[1600]\ttraining's rmse: 0.799467\tvalid_1's rmse: 1.04512\n",
      "[1700]\ttraining's rmse: 0.790525\tvalid_1's rmse: 1.04491\n",
      "Early stopping, best iteration is:\n",
      "[1295]\ttraining's rmse: 0.826043\tvalid_1's rmse: 1.04473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_splits, random_state=1337)\n",
    "\n",
    "\n",
    "oof_train = np.zeros((X_train.shape[0]))\n",
    "oof_test = np.zeros((X_test.shape[0], n_splits))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_index, valid_index in kfold.split(X_train, X_train['AdoptionSpeed'].values):\n",
    "    \n",
    "    X_tr = X_train.iloc[train_index, :]\n",
    "    X_val = X_train.iloc[valid_index, :]\n",
    "    \n",
    "    y_tr = X_tr['AdoptionSpeed'].values\n",
    "    X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n",
    "    \n",
    "    y_val = X_val['AdoptionSpeed'].values\n",
    "    X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n",
    "    \n",
    "    print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n",
    "    \n",
    "    d_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    d_valid = lgb.Dataset(X_val, label=y_val)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    \n",
    "    print('training LGB:')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    \n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    oof_train[valid_index] = val_pred\n",
    "    oof_test[:, i] = test_pred\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  19.,  225., 1341., 3096., 3607., 3060., 2174., 1118.,  332.,\n",
       "          21.]),\n",
       " array([0.76368075, 1.12249949, 1.48131824, 1.84013698, 2.19895572,\n",
       "        2.55777447, 2.91659321, 3.27541195, 3.6342307 , 3.99304944,\n",
       "        4.35186818]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD3CAYAAAAaEj9YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(oof_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Counts =  Counter({4.0: 4197, 2.0: 4037, 3.0: 3259, 1.0: 3090, 0.0: 410})\n",
      "Predicted Counts =  Counter({2.0: 3977, 4.0: 3793, 1.0: 3692, 3.0: 3531})\n",
      "Coefficients =  [0.45737209 2.09710118 2.48984622 2.89432827]\n",
      "QWK =  0.44252125082272553\n"
     ]
    }
   ],
   "source": [
    "# Compute QWK based on OOF train predictions:\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(oof_train, X_train['AdoptionSpeed'].values)\n",
    "coefficients = optR.coefficients()\n",
    "pred_test_y_k = optR.predict(oof_train, coefficients)\n",
    "print(\"\\nValid Counts = \", Counter(X_train['AdoptionSpeed'].values))\n",
    "print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "print(\"Coefficients = \", coefficients)\n",
    "qwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, pred_test_y_k)\n",
    "print(\"QWK = \", qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pred distribution: Counter({2: 3977, 4: 3793, 1: 3692, 3: 3531})\n",
      "test pred distribution: Counter({4.0: 1049, 2.0: 1018, 1.0: 958, 3.0: 923})\n"
     ]
    }
   ],
   "source": [
    "train_predictions = optR.predict(oof_train, coefficients).astype(int)\n",
    "print('train pred distribution: {}'.format(Counter(train_predictions)))\n",
    "\n",
    "test_predictions = optR.predict(oof_test.mean(axis=1), coefficients)\n",
    "print('test pred distribution: {}'.format(Counter(test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions.astype(np.int32)})\n",
    "submission.head()\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
